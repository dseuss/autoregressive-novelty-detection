train:
  epochs: 30
  optimizer:
    type: SGD
    lr: 0.002
    momentum: 0.9
    weight_decay: 0.0005
    nesterov: True
  scheduler:
    type: "WarmupLR"
    burn_in_steps: 100

model:
  encoder:
    type: 'ResidualAE'
    input_shape: [28, 28]
    color_channels: 1
    encoder_sizes: [32, 64]
    fc_sizes: [64]
    latent_activation:
      type: "Sigmoid"

  regressor:
    type: 'AutoregressionModule'
    layer_sizes: [32, 32, 32, 32, 100]

  loss:
    re_weight: 0.5

data:
  batch_size_per_gpu: 64
  workers_per_gpu: 4
  __common: &common
    type: 'MNIST'
    positive_classes: [0, 1, 2]
  train:
    <<: *common
    transforms:
      - type: "RandomAffine"
        degrees: 20
        shear: 30
  valid:
    <<: *common

experiment_configurations:
  - positive_classes: [0]
  - positive_classes: [1]
  - positive_classes: [2]
  - positive_classes: [3]
  - positive_classes: [4]
  - positive_classes: [5]
  - positive_classes: [6]
  - positive_classes: [7]
  - positive_classes: [8]
  - positive_classes: [9]
